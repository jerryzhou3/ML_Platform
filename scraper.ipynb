{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import collections\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "\n",
    "from pdfminer import high_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input string by converting it to lowercase, removing 's and apostrophe.\n",
    "    \n",
    "    args:\n",
    "        text (str) : the input text\n",
    "        \n",
    "    return:\n",
    "        str : the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    clean = re.sub(r'\\'s*\\s', ' ', text)\n",
    "    clean = re.sub(r'\\'s\\Z', '', clean)\n",
    "    clean = re.sub(r'\\'', '', clean)\n",
    "    return clean\n",
    "\n",
    "def tokenize(cleaned_text):\n",
    "    \"\"\"\n",
    "    Tokenize the input string.\n",
    "    \n",
    "    args:\n",
    "        cleaned_text (str): the input text, output from clean_text\n",
    "        \n",
    "    return:\n",
    "        List[str] : a list of tokens from the input text\n",
    "    \"\"\"\n",
    "    token = nltk.word_tokenize(cleaned_text)\n",
    "    token = [split for t in token for split in re.split(r'[^a-zA-Z0-9]', t)]\n",
    "    token = [t for t in token if t != '']\n",
    "    return token\n",
    "\n",
    "def lemmatize(tokens, stopwords = {}):\n",
    "    \"\"\"\n",
    "    Lemmatize each token in an input list of tokens\n",
    "    \n",
    "    args:\n",
    "        tokens (List[str]) : a list of token, output from tokenize\n",
    "    \n",
    "    kwargs:\n",
    "        stopwords (Set[str]) : the set of stopwords to exclude\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of lemmatized and filtered tokens\n",
    "    \"\"\"\n",
    "    def tag_to_part(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return 'a'\n",
    "        elif tag.startswith('V'):\n",
    "            return 'v'\n",
    "        elif tag.startswith('R'):\n",
    "            return 'r'\n",
    "        else:\n",
    "            return 'n'\n",
    "        \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    parts = list()\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        tag = nltk.pos_tag([tokens[i]])\n",
    "        part = tag_to_part(tag[0][1])\n",
    "        parts.append(part)\n",
    "        \n",
    "    lemmatized = list()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        lemma = lemmatizer.lemmatize(tokens[i], pos = parts[i])\n",
    "        if lemma not in stopwords and len(lemma) >= 2:\n",
    "            lemmatized.append(lemma)\n",
    "            \n",
    "    return lemmatized\n",
    "\n",
    "def preprocess_text(text, stopwords = {}):\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    return lemmatize(tokens, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "PATH_TO_CHROMEDRIVER = \"C:/Users/jerryzhou3/Documents/chromedriver.exe\"\n",
    "\n",
    "def init_chromedriver(debug = False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if not debug:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n",
    "    return webdriver.Chrome(executable_path = PATH_TO_CHROMEDRIVER, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_nature(url):\n",
    "    \"\"\"\n",
    "    Parse a single New York Times article at the given URL\n",
    "    \n",
    "    args:\n",
    "        url (str) : the article URL\n",
    "    \n",
    "    return:\n",
    "        Dict[str, str] : the parsed information stored in JSON format, which includes:\n",
    "            Title, Author, Published Date, Summary and Content\n",
    "    \"\"\"\n",
    "    month_to_num = {\"January\": \"01\", \"February\": \"02\", \"March\": \"03\", \"April\": \"04\", \"May\": \"05\", \"June\": \"06\", \"July\": \"07\", \"August\": \"08\", \"September\": \"09\", \"October\": \"10\", \"November\": \"11\", \"December\": \"12\"}\n",
    "#     driver = init_chromedriver()\n",
    "#     driver.get(url)\n",
    "#     soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "    date = soup.find(\"time\", {\"itemprop\": \"datePublished\"}).get_text().strip().split()\n",
    "    year = date[2]\n",
    "    month = month_to_num[date[1]]\n",
    "    day = date[0]\n",
    "    publishedDate = year + '-' + month + '-' + day\n",
    "    title = soup.find(\"h1\", {\"itemprop\": \"headline\"}).get_text().strip()\n",
    "#     author = soup.find_all(\"span\", class_ = \"block hide-overflow nowrap overflow-ellipsis\")\n",
    "    author = soup.find_all(\"h3\", {\"data-tooltip\": \"Show author information\"})\n",
    "    authors = [re.sub(r'&', '', a.get_text()).strip() for a in author]\n",
    "    summary = soup.find(\"div\", class_ = \"article-item__teaser-text serif\").get_text().strip()\n",
    "    content = soup.find(\"div\", class_ = \"article__body serif cleared\").find_all(\"p\")\n",
    "    content = [c.get_text().strip() for c in content if c.find(\"aside\") is None]\n",
    "    content = [c for c in content if len(c) > 0]\n",
    "    parse = {\"Title\": title, \"Author\": authors, \"Published Date\": publishedDate, \"Summary\": summary, \"Content\": content}\n",
    "    return parse\n",
    "\n",
    "def extract_nature_articles(start_date, end_date, base_url = \"https://www.nature.com\"):\n",
    "    \"\"\"\n",
    "    Search for and parse all coronavirus-related News article from the Nature journal that were\n",
    "    published in a given period\n",
    "    \n",
    "    args:\n",
    "        start_date (str): the lower bound of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "    \n",
    "    return:\n",
    "        List[Dict[str, str]] : a list of parsed JSON for each articles returned by\n",
    "            the search query\n",
    "    \"\"\"\n",
    "    month_to_num = {\"January\": \"01\", \"February\": \"02\", \"March\": \"03\", \"April\": \"04\", \"May\": \"05\", \"June\": \"06\", \"July\": \"07\", \"August\": \"08\", \"September\": \"09\", \"October\": \"10\", \"November\": \"11\", \"December\": \"12\"}\n",
    "    search_url = \"https://www.nature.com/search?title=coronavirus&order=date_asc&article_type=news&journal=nature\"\n",
    "    search_url += \"&date_range=\" + start_date[:4] + \"-\" + end_date[:4]\n",
    "    articles = list()\n",
    "    soup = BeautifulSoup(requests.get(search_url).text, \"html.parser\")\n",
    "    script = soup.find(\"script\", {\"data-test\": \"dataLayer\"}).string.strip()\n",
    "    script_dict = json.loads(script[script.index('=') + 1:  -1])\n",
    "    total_pages = script_dict[0][\"page\"][\"search\"][\"totalPages\"]\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        search_url_page = search_url +\"&page=\" + str(page)\n",
    "        soup = BeautifulSoup(requests.get(search_url_page).text, \"html.parser\")\n",
    "        titles = soup.find_all(\"a\", {\"data-track-action\": \"search result\"})\n",
    "        for title in titles:\n",
    "            title_text = title.get_text().strip()\n",
    "            if \"Daily briefing\" in title_text or \"Podcast\" in title_text or \"Backchat\" in title_text:\n",
    "                continue\n",
    "            date = title.find_previous(\"time\").get_text().strip().split()\n",
    "            month = date[1]\n",
    "            day = date[0]\n",
    "            year = date[2]\n",
    "            day = '0' * (2 - len(day)) + day\n",
    "            date = year + '-' + month_to_num[month] + '-' + day\n",
    "            if date < start_date or date > end_date:\n",
    "                continue\n",
    "            url = base_url + title[\"href\"]\n",
    "            article = parse_page_nature(url)\n",
    "            articles.append(article)\n",
    "    articles = sorted(articles, key = lambda a: (a[\"Published Date\"], a[\"Title\"]))\n",
    "    return articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
